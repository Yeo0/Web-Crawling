{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1= 스크레이핑이란?\n",
      "p= 웹페이지를 분석하는것\n",
      "p= 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "#BeautifulSoup 기본\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#분석하고 싶은 HTML\n",
    "html=\"\"\"\n",
    "<html><body>\n",
    "\t<h1>스크레이핑이란?</h1>\n",
    "\t<p>웹페이지를 분석하는것</p>\n",
    "\t<p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "#HTML 분석하기\n",
    "soup=BeautifulSoup(html,'html.parser') #BeautifulSoup(html, parser종류)\n",
    "\n",
    "#원하는 부분 추출하기\n",
    "h1=soup.html.body.h1 #html.body.h1에 있는 요소에 접근\n",
    "p1=soup.html.body.p\n",
    "p2=p1.next_sibling.next_sibling #두번째 p태그 접근/ next_sibling 1번할 경우 줄바꿈 뒤에있는 공백이 추출됨\n",
    "\n",
    "#요소의 글자 출력하기\n",
    "print(\"h1=\",h1.string) #글자부분 추출\n",
    "print(\"p=\",p1.string)\n",
    "print(\"p=\",p2.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#title= 스크레이핑이란?\n",
      "#body= 웹페이지를 분석하는것\n"
     ]
    }
   ],
   "source": [
    "#id로 요소 찾는 법 \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html=\"\"\"\n",
    "<html><body>\n",
    "\t<h1 id=\"title\">스크레이핑이란?</h1>\n",
    "\t<p id=\"body\">웹페이지를 분석하는것</p>\n",
    "\t<p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "#html 분석하기\n",
    "soup=BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#find()메소드로 원하는 부분 추출\n",
    "title=soup.find(id='title')\n",
    "body=soup.find(id=\"body\")\n",
    "\n",
    "#텍스트 부분 출력\n",
    "print(\"#title=\",title.string)\n",
    "print(\"#body=\", body.string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver = http://www.naver.com\n",
      "None = http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "#여러개의 요소 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html=\"\"\"\n",
    "<html><body>\n",
    "  <ul>\n",
    "    <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "    <li><a href=\"http://www.daum.net\"daum</a></li>\n",
    "  </ul>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "#html 분석하기\n",
    "soup=BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#find_all 메소드로 추출\n",
    "links=soup.find_all(\"a\")\n",
    "\n",
    "#링크 목록 출력하기\n",
    "for a in links:\n",
    "    href=a.attrs['href'] #attrs속성에서 추출\n",
    "    text=a.string\n",
    "    print(text,'=',href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a.html'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DOM요소 속성\n",
    "from bs4 import BeautifulSoup\n",
    "soup=BeautifulSoup(\n",
    "    \"<p><a href='a.html'>test</a></p>\", \"html.parser\")\n",
    "\n",
    "#결과 확인\n",
    "soup.prettify() #'<p>\\n <a href=\"a.html\">\\n  test\\n </a>\\n</p>'\n",
    "\n",
    "#<a>태그를 a 변수에 할당\n",
    "a=soup.p.a\n",
    "\n",
    "#attrs 속성의 자료형 확인\n",
    "type(a.attrs) #dict\n",
    "\n",
    "#href 속성이 있는지 확인\n",
    "'href' in a.attrs # true\n",
    "\n",
    "#href 속성값 확인\n",
    "a['href'] #a.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "이번 예보기간에는 고기압의 영향으로 대체로 맑은 날이 많겠습니다.<br />기온은 평년(최저기온: 9~19℃, 최고기온: 22~26℃)과 비슷하거나 조금 낮겠습니다.<br />강수량은 평년(2~8mm)보다 적겠습니다.\n"
     ]
    }
   ],
   "source": [
    "#urlopen()과 BeautifulSoup 조합하기\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url=\"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "#urlopen()으로 데이터 가져오기\n",
    "res=req.urlopen(url)\n",
    "\n",
    "#BeautifulSoup으로 분석하기\n",
    "soup=BeautifulSoup(res,'html.parser')\n",
    "\n",
    "#원하는 데이터 추출\n",
    "title=soup.find(\"title\").string\n",
    "wf=soup.find(\"wf\").string\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1= 위키북스 도서\n",
      "li= 유니티 게임 이펙트 입문\n",
      "li= 스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "li= 모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "#<h1>태그 <li>태그 추출\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#분석대상 HTML\n",
    "html=\"\"\"\n",
    "<html><body>\n",
    "<div id=\"meigen\">\n",
    "  <h1>위키북스 도서</h1>\n",
    "  <ul class=\"items\">\n",
    "    <li>유니티 게임 이펙트 입문</li>\n",
    "    <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "    <li>모던 웹사이트 디자인의 정석</li>\n",
    "  </ul>\n",
    "</div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "#HTML 분석하기\n",
    "soup=BeautifulSoup(html,'html.parser')\n",
    "\n",
    "#필요한 부분을 CSS 쿼리로 추출하기\n",
    "#타이틀 부분 추출하기\n",
    "h1=soup.select_one(\"div#meigen > h1\").string #하위로 갈때 > 사용\n",
    "print(\"h1=\",h1)\n",
    "\n",
    "#목록부분 추출하기\n",
    "li_list=soup.select(\"div#meigen > ul.items > li\")\n",
    "for li in li_list:\n",
    "    print(\"li=\",li.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/krw= 1,119.00\n"
     ]
    }
   ],
   "source": [
    "#네이버 금융에서 환율정보 추출\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "#HTML 가져오기\n",
    "url=\"http://info.finance.naver.com/marketindex/\"\n",
    "res=req.urlopen(url)\n",
    "\n",
    "#HTML 분석하기\n",
    "soup=BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "#원하는 데이터 추출하기\n",
    "price=soup.select_one('div.head_info > span.value').string\n",
    "print(\"usd/krw=\",price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"value\">1,119.00</span>, <span class=\"value\">997.86</span>, <span class=\"value\">1,316.22</span>, <span class=\"value\">163.67</span>, <span class=\"value\">112.3100</span>, <span class=\"value\">1.1668</span>, <span class=\"value\">1.3148</span>, <span class=\"value\">94.1100</span>, <span class=\"value\">71.12</span>, <span class=\"value\">1644.0</span>, <span class=\"value\">1202.2</span>, <span class=\"value\">43336.01</span>]\n",
      "1,119.00\n"
     ]
    }
   ],
   "source": [
    "#네이버 금융에서 환율정보 추출\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "#HTML 가져오기\n",
    "url=\"http://finance.naver.com/marketindex/\"\n",
    "res=req.urlopen(url)\n",
    "\n",
    "#HTML 분석하기\n",
    "soup=BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "#원하는 데이터 추출하기\n",
    "result=soup.select('span.value')\n",
    "price=result[0].string #선택자로만 해결하기 보단 다양한 배열처리 + 문자열처리\n",
    "print(price)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#네이버 뉴스기사 크롤링(제목링크&제목)\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url=\"http://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=105\"\n",
    "res=req.urlopen(url)\n",
    "\n",
    "soup=BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "result=soup.select('#section_body a') #오류. \n",
    "#result=soup.find_all(\"a['href']\")\n",
    "print(result)\n",
    "for res in result:\n",
    "    print(res.attrs['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#네이버 뉴스기사 크롤링 (기사본문)\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import time #한번에 너무 많은 요청 방지\n",
    "\n",
    "url=\"http://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=105\"\n",
    "res=req.urlopen(url)\n",
    "\n",
    "soup=BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "results=soup.select('div#section_body > ul.type06 > a')\n",
    "print(result)\n",
    "for result in results:\n",
    "    url_article=result.attrs['href']\n",
    "    response=req.urlopen(url_article)\n",
    "    soup_article=BeautifulSoup(response,'html.parser')\n",
    "    content=soup_article.select_one(\"#articleBodyContents\")\n",
    "    print(content.contents) #string하나 말고 전체 가져올 때 (string 안되면 contents 가져온다 생각 )\n",
    "    time.sleep(1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#네이버 뉴스기사 크롤링 (기사본문_가공)\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import time #한번에 너무 많은 요청 방지\n",
    "\n",
    "url=\"http://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=105\"\n",
    "res=req.urlopen(url)\n",
    "\n",
    "soup=BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "results=soup.select('div#section_body a')\n",
    "print(result)\n",
    "for result in results:\n",
    "    url_article=result.attrs['href']\n",
    "    response=req.urlopen(url_article)\n",
    "    soup_article=BeautifulSoup(response,'html.parser')\n",
    "    content=soup_article.select_one(\"#articleBodyContents\")\n",
    "    \n",
    "    output=\"\"\n",
    "    for item in content.contents:\n",
    "        stripped=str(item).strip() # 공백 제거\n",
    "        if stripped ==\"\":\n",
    "            continue\n",
    "        if stripped[0] not in[\"<\",\"/\"]: # <나 /로 시작하지 않을때만\n",
    "            output+=str(item).strip() #태그이기 때문에 문자열 변환 해줘야 함\n",
    "    print(output) #print(output.replace(\"dslkkljd\",\"\"))\n",
    "    time.sleep(1 )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
